{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![image.png](attachment:574885a1-13f3-4a14-ae81-792b120e6f43.png)\n\n# Masked Language Model.\n\nIn MLM we input a sentence and optimize model weight to get the same output. But before sending input we randomly masked some tokens with [Mask] token. To train the MLM model we need to follow few steps\n* Tokenize input.\n* Create a label with input or clone input tensor.\n* Randomly masked some token in input.\n* Initialize the model and calculate the loss.\n* Finally update weight.","metadata":{},"attachments":{"574885a1-13f3-4a14-ae81-792b120e6f43.png":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAT4AAACfCAYAAABgD7XPAAAbYElEQVR4Ae2dv6slx5mG5x9QIBAIFsEgDYIJhEAIOXKghcvAhQmk0A5klAx25kSCTYxAya4iKTKMkkmUWeBA0W4mlo1WgXCkZBMlKxaNx7I11r3e3V6emvPerVvqH9V9qs7p0/U2NH1On+6u73ur6umvfnSfG50XK2AFrEBjCtxozF+7awWsgBXoDD4XAitgBZpTwOBrLsvtsBWwAgafy4AVsALNKWDwNZfldtgKWAGDz2XACliB5hQw+JrLcjtsBayAwecyYAWsQHMKGHzNZbkdtgJWwOBzGbACVqA5BQy+5rLcDlsBK2DwuQxYASvQnAIGX3NZboetgBUw+FwGrIAVaE4Bg6+5LLfDVsAKGHwuA1bACjSngMHXXJbbYStgBQw+lwErYAWaU8Dgay7L7bAVsAIGn8uAFbACzSlg8DWX5XbYClgBg89lwApYgeYUMPiay3I7bAWsgMHnMmAFrEBzChh8zWW5HbYCVsDgcxmwAlagOQUMvuay3A5bAStg8LkMWAEr0JwCBl9zWW6HrYAVMPhcBqyAFWhOAYOvuSy3w1bAChh8LgNWwAo0p0AT4Lv413/pLv7935rLXDtsBaxAvwKbBx/Ae3h20+Drz3/vtQJNKrBp8P3tD190D89vd9/+/d8ZfE0WbzttBfoV2Cz4aN4CPYOvP+O91wq0rMAmwadIL4Dv7kuO+Fou4fbdCvQosDnwhT69XaT38O5LHaubuj05711WoGEFNgW+0Lx989UnTdwd9Ay+hku3XbcCAwpsBnyheXvn1o+gZ/AN5Lx3W4GGFdgE+EKkNwA9g6/h0m3XrcCAAicPvhDp0aylXy9q3saf3cc3kPvebQUaVeCkwdc3kBEDT58NvkZLt922AgMKnCz4QvN2ItIz+AZy3butQOMKnCT4xgYyBLt464iv8VJu961AosDJgW9qICMGXvi8m9P331//R+K6v1oBK9CqAicFvtCn1zNP70ewiwc5zm52P/zz7xfl7/8+/kvH2l1eLDrfJ1kBK7BOBU4GfLkDGTEEaeI+/uS3s5UnOvz+/gfdo1++0T16+6z77je/WgzP2Yn7BCtgBaorcBLgmzOQcQW+s5vd9w8+mi0gaQFMXmWllxw8vHMr7AOAIQKcfVWfYAWswJoUWD345g5kAD7AtQR6RHoPR5rS4br3P1hT/tkWK2AFFiiwavDNHsigb49IbyGc/vyP7z6J9OI+wuTztz+54T6/BQXNp1iBNSmwWvAtHchYEukpQ/74s5+OPgGiaBLbvFgBK3C6CqwSfKF5m7xa6qrvLonAtL9EM3SsmRunQyTqxQpYgdNVYHXgC83bmdALzdsFAxlptn337i86BjIEub4tTd3/+a//TE/1dytgBU5IgVWBb3GkVwB65NnViO5AVAkUmeLixQpYgdNWYDXgWzpPb+lARm+2XV50f/novY6o7trbXohAmd7y5qudnwDpVc47rUD3ww8/dF988UX32WefdR9++GH34MGD1aqyCvAdYyBjLEeY9MzE5dDs3QGPOXyG3phq/q11Bb7++uvu2Wef7Z5++unuxo0b3XvvvbdaSY4OvsXN24VTVnJzgn48bAPKBl6uaj6uZQUAH9B78cUXAwDff//91cpxVPAdep7eanOhgmE0Ob788svJ9auvvupYKbSPHj2qYMnyS2JXjg81jvnmm2+WG97omZShZ555xuAby/8Q6TGIQP/Z0GBCsn/pExljdtT4jZciHDtKpKnxwgsvjK7cmV9++eXulVde6V5//fXujTfe6O7du9fdv38/wJA+m2Mu2PPcc8+N+jDl45Lfn3rqqVX3Tx0zT8bSNvjG1Om60IQMsJsLvcrN2wmzs37+6+8/CYMjj+7dPepzvTngi6Hw/PPPd6yARn00wJCO6mMB0ODLKnKrOcjgG8kKIqGcJySuRYELXzgwYkaVnwL0eMEBkerZzfBWl2O91ADwEdGxAjRgRiQTr+yjaUKHtCIrnaPz+P38/DyM1lURbeSigA8bsW3OGvsA3LlGn/+xFvFntFvziOSIZEf9yeAbkv/y4v9HS5Nm7DXQRb+VeCJjyJyS+4FeGAWOolhsZ3rMMRaBD+gBLipy30qzlk5omrg0ewGAYCOAAB2u8/nnnx/UFaZEvPPOO7NW/JDdQI+otc/vqX30L3qZp4DB16fX5UWX8xKAawDc44UDfSbU2qfm7TXbd/AGfo9/d/j5TAIf0AIGuQsDHJ9++mn31ltvBQgCD0V/RIaHhl+u3TqOQQk14YE1UaOXwyhg8PXoHCIi5sRF0dzY51MZyLiK9Mb8Ort58MGOpeBT1tGvBwAV7Ql+RIXH6vOTbWNbKh/AA34G35hS5X8z+BJNmRN39VLPMUBEUVLRJzISe0p9DZEefXpR87YX5ndudTwHfMj+vn3BJ40Y3BBIgB9NYZrHa12ofI74jpM7Bl+i+6wm7ok0b3GRV2CFR9kyYb7kNfiJlNlfS4GPBD/++OMAPEV99Jutbc6fhDH4pMThtwZfpDlPPtBs7Y2EEmBwHJA8pT/3yYb6Lio81JtdSoIPyAE99fdxbSYNr3Ex+I6XKwbfTnuadsxnm2wKAsBTffPJ5UVoxoYR3QTkP4L92c2DjfKWBB/ZycgvgxsAkFHftTZ3DwE+BlAY5KEPlJFnnkll5TNdA9wU9u0H5WbDdUgHn/oWfic9RqixZezYvvO1r88f8ps85ro8BTRkg67Bdg74lCb2kwbbpfbHNuR+rvrIGk8wZDUFz2+HuX3HftohV7T0uNCHyXv8pvr6AOOBBjpKg4/CCfAAHwBkmkm8MHqqlRHhnIoSn9/3mcqga7Ll+9RCuuqTLD24AWjw+7XXXguDPvR3ah4kmvCZfWjEFCLAQQWfu6A13QkMJJGPqd8AAj1Ih/S0YgO2oX/OdByOyfEHHbkux5L2ENSnwMd5QJRZBvjHwJlsZ8t30uF3tK65VAVfzhuNiYp4DdTS/76tKc6ca2dD/kBRX2nwUfkU8fVNkaFSqPCSNpV33wV4kCbpUclzKjOVr/TgBtEXER3+YQ8gwB6NHGMf+9L9HE9FnqsFERzncn3SAzZaiMQALGmq6wFbtLIP/emXHVrwh/ziRhb7E/vEdeLr4xtpcm3ACsDSZQx8gOztt98O50uv+Pqyn3SwiXRS4Kfp7fO9GvgmX+qpZiEjnr/51T4+rObcrP6+A0W3FBwKE4Vszjy+ITGnwEdFUGWh8HJH32cBctg+14fS4MMv4AVsVDmxCzDxHTgDAiIwjhNMVKnRgrwAnEORUqoT4Iuja4EP6HEt6QwgsIN9bAUxbB26SQCg1B9sJD1WPbfNMemEdvlEejTr06UPfPhM5Ms5Ap7sxmZpqc/SmLQ4Z+5NI7Vp6Hs18P35n/5h8jXuoQ/szq3w7O6Qgae0n6Z6TtOeQRymwtRcKDRzoTFmDwVQlZHCSsQQLxRwKj+/Kd2hyhefN/SZyiLY4Isq/9Dx2l8SfNgvX9gqGlGzm7RoyhJBsfKdc4Ac0KCCcx4r2qWayeZ0G4MPPfGdGw/XAAhcl+sDQn4DZvzOd4DFMX0LtvG78kjX4iaF3vwun9iycm3sUd6SNvnBsenC8coz0kAHfGYf3zkXsLIfu7mGVsoXaXAMdklv/FzSXZDaln6vAr7Q5zXy/7RXnf5nN5+M4qZWnfB35h9Owu/8dnh0r6abpcFHpaJQUiApyH13fDXRxo7J8RmIUhkBDSufc6MlKh/nUHnYUpmWLFwnhgQVl0qbG4FQWdWcVEUmqunTLbUvBh/nAiVswR+uATgA7dDS1wzFHiAi6Gk7pzkJpEgbW/rSj8FHGRDsADZ5iHZT+Yivsk3liH2llyrgu5rYq+bs0Pb89sGfaCgtYHq9EPVlDHLQr1lzMKck+CisqngUxqEIjMrF74LOUAVJNUu/Ew3IfiA7p+BT+UhfNiwFH90DAj3AwZe+KCe1Pf0eV2RsAlxT14nBh56krXP5bcmiyIvr4Q8gmrJjKJ2hCCwFHzYDMdKeAl6cFnCNtZ9z44uvM/a5Cvj+9OufT49w3rm1uWhPQmf9W1vlZ3gFDgrevn18MYgozGPNj7jQDgFSOg1tqSgq+IBvTgUtAT78VfqqvH1R1JD96X40wQ+gw3Xxb2xJwYcNc5rK6bWxXf5gA/kyR9P0ekPfU/CRzhJQc53U3iHYDtkytb84+ELEQzN3KMrb7aefiwGQLS5Zk7Y1qHN5UUWCUuCjgsTRHhWYZu/QEleyJdCl0Cs9zp8bsZUAH2mSNpAgQhsbIR3SId3PzYJIKwekKfikw5yoKU6fARj5A1CWwCi+3tDnGHykA/CXLPjJgJFspizvc+Pps6E4+K5eJ58BvlN6QqNPvKF99HGGPysaa/Lu+vk4tsZSAnzcZeNKQ6XluhTwsUXg4ng+z4ku6AdSdAR05vRBYdO+4ON8aacmYYloI/Zr6uaRgg8diEKXLGiv6Al/yM++/rkl107PScE3doNMz02/x33KS8pBer30e3Hw8TzqZOf+hpu5Epj38E3pQNTLK/hrLKq83DXnNnWp6FQ+Ih7ddRX95HTOx8/20kSbE2FQMamgS6FD5eNcoMt2bsSIf9iMvwAqx9+c/NNNRLZx/aElBt+S/Iuvy7V0IynpT5yGPpcEH7rLbsCXO6gkW6a2xcHHnLypx7eo8Kc+YXlKWPzDz7EmP7/XmtaSgo/mg1bu+FqpkBRYIgMKF/1PNMuIEoAHFVTQy72Dcz1GQAUfor6chfNkN+nnphdfm2uQrtKeCz5uEoI9tpRsYsV9l1TmoUgyBd/SaA9dSDP2h+kptRa0F6yW5p9sY1BI11o/+C4vsl8rXyvSkXDH3tLXGf6YfKzJX/FNNAIIAAA8VGhmztN3ogm3RFeMmAE6ChcrlYRzBDy+cy0gNKePKa1wOc1dCruiLdLMOSfN533Ax80AUBKV4T82lFwAGhrrRjIEtBh82LIUVvijCJo8JS/ZV2tpFny8lGAqyuF5Vvq/avVt1crUJded1GL3nr4l1546R+CjkqnQU/C1UqG0xqDjePYLeEByqIKO2cA5soG7/1RHd1xJSXtupCZb9gEfoOUmgR7STNctsQVg0gTADzXfSoJP+U+e1pgWEuvSLPiyohzA98tlk0pjkU/hc+4ARw1fVMEo+IKZQJduAQ1wUsSnCBF4zYnyUj9o7iotKt1Q047zaFIqGqKJMwSFNI30+77gI/oFetid20RPbRj6jm3Kl7G+z1LgQ281F/GHm8k++Tnkl/Y3Cz6ar1lRzkaezVWGD20n+zt3z+0Onb/PflUwgQ/w9K00hWgCE5EBG2A3Bqg5NqVN17ERWvWtAR2As9QGKh+VXPCaEzkS8cUREtqUXIhqlS+HAp/S4+ZG9G7wPcnRop0YWS8maGBEV5VlcmSX6S5vvlrllfQq8LrT1+zbkb/pFpAoghprvgI5HUfkSf/g0qVkxMeNouSSRnxDUa0jvi48raNolZbAkFZL86co+HJGMpnicay/XFwq0tLzJp/b3Tj40I0oA+gRgRFN9Q1YUKhVyAH2PiOp+4CPc+M+PiBccsEv3ZCI+IYi4FLg42YXR7D4VvMGiH7KR7RbMiovvU9qVDcXfKfwR0LKgH22k3Mad+CrMdCjCnbMiA/t4kEOKgUFOl5oegmOigr3aY7tAz6gkI7qlgRFDDSimL6bANrEx5F/JUZ1ASBlYmkXQpxnQ58NvokpHI74Xnoyv6+BiA9w0GSkArPyOQYblUWQBoxU+n2WfcBHuuprBBTACXCXWuIpPkR8Q0sp8HF90iT6kj9DUeaQLXP2Nws+9/FdLyY5fXx//NlPq/y5kmBy7IgPReJZ+NgVN2XT3/aNSKh8+LxkcANb4wEZgDE1Ded6jg9/wy/dABTZDh1dEnxcS81P0gXstZZmwedR3etFag2jumsAHxVCU1WofBq8IBrUBNtSlZK0gN5S8HG+bhq6xlCT9Hpuj3/jMT6iPCKvsf49rlISfPijiA9/SD++8YxbPe9X0hJkm+rj8zy+6wUlax7fvbvXTyr0TZV3DeDDJQAH3LBH01WogFQQKiQwKNGs3Bd82Mr0HmwFEtgnUC/NGpr2XE8gJfIbi2xLgg+b6beUP2zpU62xNAs+P7lxvTjlzGkM/yN8/bQi39YGPvqWFPWx5bv6vErCuQT4ALJAJfgBoyUL0KN5qaiLfJmamlEafLrB4AsrNpR6+UKsSbPg4zVT9Fnl/M2in9V98leTjPzWWNYGvriPC9ARAaoiEu2VqoglwEd+AGU124jUANcUsNJ8BHpcR9ADpjlPT5QGH3YxtURNbXTHFjSPB5pS+/u+0z1BZN53XtPgm+zX4u8k/XaWMKqLDrVexro28FGJGCgQBICfwIetVJoSC9fh2mpWznlyI04fUKt5jp2CHz7kTHFhCgppC56ABrty+gtrgA+b6WLADvkDCGnW50yXAXSAH5/Irz4N2gVf13WTc9eY6qK3D8clbWOfw7/Mnd0cfS0Vr6yiX7TGskbwUelll6BHRSw50kjlA1L7go88AX7AQrDGZj4DDGwm8gEa+MXKZ+AAMGnOCzJseW45BzCkWwN8XBdt6F+M/cE2bMVPIkB8omnMymf2ATt8ZuV48pDf06Vp8OW+gZknOFp/AzP/TUK/aI1FgCHKoOD23aFrpDt1TWwREASSknPLSoIPX7gegwFUemAaAxtgoC9Q45E7QYR9HKcokd9zIj1pVwt8sT/YHvuDzUSn7I9X9skffOIzZQsb06Vp8PEUAs+fTvXz1WzmpRly6O9Z/7lR8V18+EvhpLJSiIlA1gI+IKeIg0pEpNHXX7Q0z1T5FJnh+74L9lHRqfjoCeCwHXD0rfyO9thAxDRXe9Ii/zgf8ORGinP8ZL4isE79Edi1xT98xSds4hyi2r48Q3uO0Q1gnzmQ2CcN2M7tX53Souizukqs+X9Zy3wLda3+PfKBCkfhYR0qqMqvQ25pPlExqFhU6n2e5+yzm+YpAwpa0aHUAsDQkghQk5EBBxVTAAHk/I7u2LJkQSPZzxag1FjwB8jSz9fnD+AFdETp2IFdfcCTbfiL71r3+ZMmjfpLB9IuuVQB3+PfPZh+PdXusbZafVwlRZpzLfyZinbp2+PtzFtt6o/pBeioUIqe5jQBx6576N+AEZEYFVIr/V6n7o/69vBJ/syNWA+dF0vSqwK+3OYu/Xy15rEtEaPEOZOPqQH8Dfqdox0ViMgI6BH1MUAwFkHkXNPHWIElClQBH4YAtKk/HQp/xHN+u9rI5hJB9jknRHt3bk2O5KIL/YCtLTQTBT62pfttWtPT/i5XoBr4sl5YsLGpLTlzGMN/jvDq/Up/JL68KNQ9k8gufhSMfjFHe3U199WHFagGPpLMGd0N/V0VJ/IOu172F95FmBPhtjB5u09Z+sM0EkofX8m5e33peZ8VGFOgKvj4z1gqemjS7gYzej/v/nviVAc6mIsXBjR4v96En7y4oNbcvbGMPvZvDGowikv/HuArPUp3bP+c/mkpUBV8VHD+US1nlJNo6VGlN5VUzZLLi+67d3/haG9E5HRQg+kRbuaOCOafqitQFXxYn93Xt3uGN7yd+YT6v7JGcXd9mScJ9gJFkLl0ivYY1Oib9V8gGV/CCmQrUB18WJIbEam/7/sHH2U7cMwDsTOrKb8DX4sjucxr09MN9PHx+NYW54Udsxw67fkKHAR8YZoHlX+qD2zXPwZMwh8SrTjymwO94M+JwHxuEWK2/tCTBeyP33BC1OcpLHMV9vE1FDgI+DB8ztMcV5Hf/Q9WOe0j980rYaBDfxq+YojvU7B4pEhv9uAxI0ZvWWnO8hiUHk9jS9+eFyuwBgUOBj7mrYVJzRmvatLIKJESgyM1/n5xifhErjTbs5u3uwj3VEerpzRigEJvJWGklv475uex8pA+TVs9pQEchyLDqXT8uxUorcDhwNd1YRpH7huaBT9Gezmn5gP9OaKSfrA958mMXZOdR9OY37fVhSkpQE6AA3LpChCBI1GgFyuwFgUOCj6cntvfp+YiURZPRhw6eiI9ItUQ5WX2UWIzx5/KIM3Swkj/Hm8hIZpTpEfUF0d+Nd8ustRun2cFDg4+JOf/NgS0q8hOUdLYlmhr94B/bQByfaaqBODNifKw/+xmOLel4gUEiQAZvGDls+fqtVQCTsvXo4APiWg6Aojckd5rgNz1ExKJhSZwqYGDy4vw8oDQF8nLVGf0R8o+QHlqcxFPq8jaWiuwvwJHAx+mB/gRIc1oQgowYXvn1tVAAyOt9KcRTeY+EsZxHM95YSLyrokanrldYNMV9PbPF1/BCliBigocFXz4FZq9RFdzm5NxkxhI7ZrBAJFBCEaD6RMEiMwJ1Mp39vMURRis2DVNF0efsuPOrc336VUsh760FTioAkcHH97Sn8bD+6E/TSDZZwsIBUMBkWYrK99Zdcw+6XDu+e3wNuUtj94etEQ6MStwAAVWAb7g5+VFiMoC/IDSvkA6wPnYSmRZe6DlAOXASViBphRYD/h2shM5hSbogoGFg8FyFzXSL7iWydVNlVo7awX2VGB14MMfBh3okwvRH5A5QPSWlQbN2l2U1+ILB/Ysaz7dCqxGgVWCT+ow8HH1OvdjAlDAu3e34+WqXqyAFThtBVYNPklLdHX1jCxN4EP0Ae4GR0LUefelADw3a5Uj3lqB01bgJMAniRlE4DEwpqIEIAmCpUAYw+7NV8Ojam7SSn1vrcB2FDgp8El2TTymH/BPv/75kz810mTm3KkqO8gxxUVRHUDV0yCO7qS2t1ZgewqcJPjSbCASJDLjnX+ASzAEaN/+5Mb1dffnR0CO/sPHn/w2PEFCf2Jrf/mY6ujvVqAVBTYBvqnMIkJkNdimlPLvVqANBZoAXxtZaS+tgBXIVcDgy1XKx1kBK7AZBQy+zWSlHbECViBXAYMvVykfZwWswGYUMPg2k5V2xApYgVwFDL5cpXycFbACm1HA4NtMVtoRK2AFchUw+HKV8nFWwApsRgGDbzNZaUesgBXIVcDgy1XKx1kBK7AZBQy+zWSlHbECViBXAYMvVykfZwWswGYUMPg2k5V2xApYgVwFDL5cpXycFbACm1HA4NtMVtoRK2AFchUw+HKV8nFWwApsRgGDbzNZaUesgBXIVcDgy1XKx1kBK7AZBQy+zWSlHbECViBXAYMvVykfZwWswGYUMPg2k5V2xApYgVwFDL5cpXycFbACm1HA4NtMVtoRK2AFchUw+HKV8nFWwApsRgGDbzNZaUesgBXIVcDgy1XKx1kBK7AZBQy+zWSlHbECViBXAYMvVykfZwWswGYUMPg2k5V2xApYgVwFDL5cpXycFbACm1HA4NtMVtoRK2AFchUw+HKV8nFWwApsRoH/A0w8z63mK9m6AAAAAElFTkSuQmCC"}}},{"cell_type":"code","source":"!pip install torch","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:38:01.514837Z","iopub.execute_input":"2022-01-19T10:38:01.515175Z","iopub.status.idle":"2022-01-19T10:38:07.231126Z","shell.execute_reply.started":"2022-01-19T10:38:01.515144Z","shell.execute_reply":"2022-01-19T10:38:07.230132Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer and Dataset\n\nWe will be using BertTokenizer from transformers and create our own PyTorch Dataset class.","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_basic_tokenization = True)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:35:58.884491Z","iopub.execute_input":"2022-01-19T10:35:58.884819Z","iopub.status.idle":"2022-01-19T10:36:05.407006Z","shell.execute_reply.started":"2022-01-19T10:35:58.884787Z","shell.execute_reply":"2022-01-19T10:36:05.406218Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\n\ndef data_collate_fn(dataset_samples_list):\n    arr = np.array(dataset_samples_list)\n    inputs = tokenizer(text=arr.tolist(), padding='max_length', max_length=30, return_tensors='pt')\n    return inputs\n\nclass MyDataset(Dataset):\n    def __init__(self, src, tokenizer):\n        self.src = src\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.src)\n\n    def __getitem__(self, idx):\n        src = self.src[idx]\n        return src","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:38:14.754878Z","iopub.execute_input":"2022-01-19T10:38:14.755205Z","iopub.status.idle":"2022-01-19T10:38:14.761441Z","shell.execute_reply.started":"2022-01-19T10:38:14.755170Z","shell.execute_reply":"2022-01-19T10:38:14.760583Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Model\n\nWe are using model code from PyTorch [LANGUAGE MODELING WITH NN.TRANSFORMER AND TORCHTEXT](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) tutorial.","metadata":{}},{"cell_type":"code","source":"import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\n\nclass TransformerModel(nn.Module):\n\n    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n        super(TransformerModel, self).__init__()\n        self.model_type = 'Transformer'\n        self.pos_encoder = PositionalEncoding(ninp, dropout)\n        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout, batch_first=True)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        self.ninp = ninp\n        self.decoder = nn.Linear(ninp, ntoken)\n\n        self.init_weights()\n\n    def generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src, src_mask):\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, src_mask)\n        output = self.decoder(output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:38:15.100132Z","iopub.execute_input":"2022-01-19T10:38:15.100453Z","iopub.status.idle":"2022-01-19T10:38:15.115604Z","shell.execute_reply.started":"2022-01-19T10:38:15.100422Z","shell.execute_reply":"2022-01-19T10:38:15.114741Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def train(model, dataloader):\n    model.train()\n    epochs = 500\n    total_loss = 0\n    criterion = nn.CrossEntropyLoss()\n    optim = torch.optim.AdamW(model.parameters(), lr=0.0001)\n\n    for epoch in range(epochs):\n        for batch in dataloader:\n            optim.zero_grad()\n            input = batch['input_ids'].clone()\n            src_mask = model.generate_square_subsequent_mask(batch['input_ids'].size(1))\n            rand_value = torch.rand(batch.input_ids.shape)\n            rand_mask = (rand_value < 0.15) * (input != 101) * (input != 102) * (input != 0)\n            mask_idx=(rand_mask.flatten() == True).nonzero().view(-1)\n            input = input.flatten()\n            input[mask_idx] = 103\n            input = input.view(batch['input_ids'].size())\n\n            out = model(input.to(device), src_mask.to(device))\n            loss = criterion(out.view(-1, ntokens), batch['input_ids'].view(-1).to(device))\n            total_loss += loss\n            loss.backward()\n            optim.step()\n    \n        if (epoch+1)%40==0 or epoch==0:\n            print(\"Epoch: {} -> loss: {}\".format(epoch+1, total_loss/(len(dataloader)*epoch+1)))\n\n\ndef predict(model, input):\n    model.eval()\n    src_mask = model.generate_square_subsequent_mask(input.size(1))\n    out = model(input.to(device), src_mask.to(device))\n    out = out.topk(1).indices.view(-1)\n    return out","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:38:15.279227Z","iopub.execute_input":"2022-01-19T10:38:15.279552Z","iopub.status.idle":"2022-01-19T10:38:15.290448Z","shell.execute_reply.started":"2022-01-19T10:38:15.279521Z","shell.execute_reply":"2022-01-19T10:38:15.289469Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Some texts for train model. Creating dataset and dataloader instances.","metadata":{}},{"cell_type":"code","source":"text = [\"Don't speak ill of others.\",\n\"To speak ill of others is a great crime.\",\n\"Rather rectify yourself through self-criticism.\",\n\"In this way, if you rectify yourself, others will follow you.\",\n\"To speak ill of others gives rise to more problems.\",\n\"This does not do any good to society.\",\n\"More than 80 percent people of our country live in villages.\",\n\"Most of them are poor and illiterate.\",\n\"Illiteracy is one of the causes of their poverty.\",\n\"Many of the villagers are landless cultivators.\",\n\"They cultivate the lands of other people throughout the year.\",\n\"They get a very small portion of the crops.\",\n\"They provide all of us with food.\",\n\"But in want they only starve.\",\n\"They suffer most.\",\n\"The situation needs to be changed.\",\n\"We live in the age of science.\",\n\"We can see the influence of science everywhere.\",\n\"Science is a constant companion of our life.\",\n\"We have made the impossible things possible with the help of science.\",\n\"Modern civilization is a contribution of science.\",\n\"Science should be devoted to the greater welfare of mankind.\",\n\"Rabindranath Tagore got the Nobel Prize in 1913 which is 98 years ago from today.\",\n\"He was awarded this prize for the translation of the Bengali 'Gitanjali' into English.\",\n\"This excellent rendering was the poet's own.\",\n\"In the English version of Gitanjali there are 103 songs.\"]\n\ndataset = MyDataset(text, tokenizer)\ndataloader = DataLoader(dataset, batch_size=4, collate_fn=data_collate_fn)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:38:15.639077Z","iopub.execute_input":"2022-01-19T10:38:15.639403Z","iopub.status.idle":"2022-01-19T10:38:15.645526Z","shell.execute_reply.started":"2022-01-19T10:38:15.639365Z","shell.execute_reply":"2022-01-19T10:38:15.644336Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Initialize model","metadata":{}},{"cell_type":"code","source":"ntokens = tokenizer.vocab_size # the size of vocabulary\nemsize = 200 # embedding dimension\nnhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\nnlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\nnhead = 2 # the number of heads in the multiheadattention models\ndropout = 0.2 # the dropout value\nmodel = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:38:15.959211Z","iopub.execute_input":"2022-01-19T10:38:15.959553Z","iopub.status.idle":"2022-01-19T10:38:16.160218Z","shell.execute_reply.started":"2022-01-19T10:38:15.959521Z","shell.execute_reply":"2022-01-19T10:38:16.159337Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Training Model\n\ntrain(model, dataloader)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:38:16.543116Z","iopub.execute_input":"2022-01-19T10:38:16.543447Z","iopub.status.idle":"2022-01-19T10:38:16.592913Z","shell.execute_reply.started":"2022-01-19T10:38:16.543415Z","shell.execute_reply":"2022-01-19T10:38:16.590579Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Predict\n\nprint(\"Input: {}\".format(text[0]))\npred_inp = tokenizer(\"Don't speak ill of [MASK].\", return_tensors='pt')\nout = predict(model, pred_inp['input_ids'])\nprint(\"Output: {}\\n\".format(tokenizer.decode(out)))","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:35:52.762637Z","iopub.status.idle":"2022-01-19T10:35:52.763337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n* [Masked-Language Modeling With BERT](https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c)\n* [LANGUAGE MODELING WITH NN.TRANSFORMER AND TORCHTEXT](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}